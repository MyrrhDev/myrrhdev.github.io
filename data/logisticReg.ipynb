{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to upgrade packages\n",
    "# !pip3 install pandas --upgrade --user --quiet\n",
    "# !pip3 install numpy --upgrade --user --quiet\n",
    "# !pip3 install scipy --upgrade --user --quiet\n",
    "# !pip3 install statsmodels --upgrade --user --quiet\n",
    "# !pip3 install scikit-learn --upgrade --user --quiet\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "pd.set_option('precision', 3)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra imports\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import confusion_matrix, \\\n",
    "                  classification_report, accuracy_score\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from numpy.random import  normal, binomial\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from statsmodels.genmod.families.family import Binomial\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(true, pred, classes):\n",
    "    \"\"\"\n",
    "    Function for pretty printing confusion matrices\n",
    "    \"\"\"\n",
    "    cm =pd.DataFrame(confusion_matrix(true, pred), \n",
    "                     index=classes,\n",
    "                     columns=classes)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Task: classifying spam mail. When there is an interest in minimizing a particular source of errors how do we change the 'cut point' for prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  Class  \n",
       "0                       278      1  \n",
       "1                      1028      1  \n",
       "2                      2259      1  \n",
       "3                       191      1  \n",
       "4                       191      1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = read_csv(\"spambase.data\", delimiter=',', header=None)\n",
    "file = open('spambase.names', 'r')\n",
    "spam.columns = [n.strip() for n in file.readlines()]\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do some basic pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2999, 51)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.loc[:,'capital_run_length_average':'capital_run_length_total'] =\\\n",
    "        spam.loc[:,'capital_run_length_average':'capital_run_length_total'].\\\n",
    "                    apply(lambda x: np.log10(x+1))\n",
    "spam = spam[spam.word_freq_george==0]\n",
    "spam = spam[spam.word_freq_650==0]\n",
    "spam = spam[spam.word_freq_hp==0]\n",
    "spam = spam[spam.word_freq_hpl==0]\n",
    "spam =spam.drop(columns=['word_freq_george','word_freq_650',\n",
    "                         'word_freq_hp','word_freq_hpl'])\n",
    "spam['about_money']=spam.word_freq_free+spam.word_freq_business+\\\n",
    "spam.word_freq_credit+spam.word_freq_money\n",
    "spam=spam.drop(columns=['word_freq_free','word_freq_business',\n",
    "                        'word_freq_credit','word_freq_money'])\n",
    "Class = spam.Class   # move the Class column to the last position\n",
    "spam=spam.drop(columns=['Class'])\n",
    "spam['Class'] = Class\n",
    "\n",
    "spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4321)\n",
    "train, test = train_test_split(spam, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit a GLM in the learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>  2009</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  1958</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>    50</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td>     nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 30 Dec 2019</td> <th>  Deviance:          </th> <td>     nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>12:51:57</td>     <th>  Pearson chi2:      </th> <td>2.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>100</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                      <td>   -5.0624</td> <td>    0.464</td> <td>  -10.904</td> <td> 0.000</td> <td>   -5.972</td> <td>   -4.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_make</th>             <td>   -0.1299</td> <td>    0.293</td> <td>   -0.443</td> <td> 0.658</td> <td>   -0.705</td> <td>    0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_address</th>          <td>   -0.1619</td> <td>    0.131</td> <td>   -1.239</td> <td> 0.216</td> <td>   -0.418</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_all</th>              <td>   -0.0884</td> <td>    0.167</td> <td>   -0.531</td> <td> 0.595</td> <td>   -0.415</td> <td>    0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_3d</th>               <td>  350.6953</td> <td>  8.8e+05</td> <td>    0.000</td> <td> 1.000</td> <td>-1.73e+06</td> <td> 1.73e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_our</th>              <td>    0.5344</td> <td>    0.120</td> <td>    4.464</td> <td> 0.000</td> <td>    0.300</td> <td>    0.769</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_over</th>             <td>    0.6943</td> <td>    0.353</td> <td>    1.965</td> <td> 0.049</td> <td>    0.002</td> <td>    1.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_remove</th>           <td>    1.6118</td> <td>    0.329</td> <td>    4.895</td> <td> 0.000</td> <td>    0.966</td> <td>    2.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_internet</th>         <td>    0.5349</td> <td>    0.148</td> <td>    3.621</td> <td> 0.000</td> <td>    0.245</td> <td>    0.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_order</th>            <td>   -0.1082</td> <td>    0.304</td> <td>   -0.356</td> <td> 0.722</td> <td>   -0.703</td> <td>    0.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_mail</th>             <td>    0.0561</td> <td>    0.090</td> <td>    0.625</td> <td> 0.532</td> <td>   -0.120</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_receive</th>          <td>   -0.5076</td> <td>    0.380</td> <td>   -1.334</td> <td> 0.182</td> <td>   -1.253</td> <td>    0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_will</th>             <td>   -0.1398</td> <td>    0.107</td> <td>   -1.308</td> <td> 0.191</td> <td>   -0.349</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_people</th>           <td>   -0.0531</td> <td>    0.334</td> <td>   -0.159</td> <td> 0.874</td> <td>   -0.708</td> <td>    0.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_report</th>           <td>    0.0120</td> <td>    0.307</td> <td>    0.039</td> <td> 0.969</td> <td>   -0.590</td> <td>    0.614</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_addresses</th>        <td>    0.8487</td> <td>    0.802</td> <td>    1.058</td> <td> 0.290</td> <td>   -0.724</td> <td>    2.421</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_email</th>            <td>    0.1439</td> <td>    0.147</td> <td>    0.978</td> <td> 0.328</td> <td>   -0.144</td> <td>    0.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_you</th>              <td>    0.1415</td> <td>    0.051</td> <td>    2.782</td> <td> 0.005</td> <td>    0.042</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_your</th>             <td>    0.1381</td> <td>    0.074</td> <td>    1.870</td> <td> 0.061</td> <td>   -0.007</td> <td>    0.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_font</th>             <td>    0.1757</td> <td>    0.279</td> <td>    0.630</td> <td> 0.529</td> <td>   -0.371</td> <td>    0.722</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_000</th>              <td>    1.0192</td> <td>    0.468</td> <td>    2.176</td> <td> 0.030</td> <td>    0.101</td> <td>    1.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_lab</th>              <td>  -12.4533</td> <td>   10.458</td> <td>   -1.191</td> <td> 0.234</td> <td>  -32.950</td> <td>    8.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_labs</th>             <td>   -1.6974</td> <td>    1.046</td> <td>   -1.623</td> <td> 0.105</td> <td>   -3.748</td> <td>    0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_telnet</th>           <td>   -0.2793</td> <td>    1.874</td> <td>   -0.149</td> <td> 0.882</td> <td>   -3.952</td> <td>    3.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_857</th>              <td>   -0.5357</td> <td>    3.463</td> <td>   -0.155</td> <td> 0.877</td> <td>   -7.323</td> <td>    6.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_data</th>             <td>   -0.4946</td> <td>    0.292</td> <td>   -1.696</td> <td> 0.090</td> <td>   -1.066</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_415</th>              <td>  424.0571</td> <td> 4.52e+07</td> <td> 9.38e-06</td> <td> 1.000</td> <td>-8.86e+07</td> <td> 8.86e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_85</th>               <td>   -1.8880</td> <td>    1.518</td> <td>   -1.244</td> <td> 0.214</td> <td>   -4.863</td> <td>    1.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_technology</th>       <td>    0.8304</td> <td>    0.547</td> <td>    1.519</td> <td> 0.129</td> <td>   -0.241</td> <td>    1.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_1999</th>             <td>    0.2841</td> <td>    0.237</td> <td>    1.197</td> <td> 0.231</td> <td>   -0.181</td> <td>    0.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_parts</th>            <td>    0.3967</td> <td>    2.682</td> <td>    0.148</td> <td> 0.882</td> <td>   -4.861</td> <td>    5.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_pm</th>               <td>   -1.1317</td> <td>    0.562</td> <td>   -2.015</td> <td> 0.044</td> <td>   -2.233</td> <td>   -0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_direct</th>           <td>   -0.3583</td> <td>    0.445</td> <td>   -0.806</td> <td> 0.421</td> <td>   -1.230</td> <td>    0.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_cs</th>               <td> -354.6653</td> <td> 3.85e+06</td> <td>-9.21e-05</td> <td> 1.000</td> <td>-7.55e+06</td> <td> 7.55e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_meeting</th>          <td>   -6.4294</td> <td>    2.328</td> <td>   -2.762</td> <td> 0.006</td> <td>  -10.993</td> <td>   -1.866</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_original</th>         <td>   -1.4478</td> <td>    1.044</td> <td>   -1.387</td> <td> 0.165</td> <td>   -3.493</td> <td>    0.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_project</th>          <td>   -1.7884</td> <td>    0.827</td> <td>   -2.164</td> <td> 0.030</td> <td>   -3.408</td> <td>   -0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_re</th>               <td>   -0.7498</td> <td>    0.195</td> <td>   -3.842</td> <td> 0.000</td> <td>   -1.132</td> <td>   -0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_edu</th>              <td>   -4.7957</td> <td>    0.948</td> <td>   -5.059</td> <td> 0.000</td> <td>   -6.654</td> <td>   -2.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_table</th>            <td>   -1.4227</td> <td>    2.442</td> <td>   -0.583</td> <td> 0.560</td> <td>   -6.210</td> <td>    3.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_conference</th>       <td>   -4.2235</td> <td>    1.523</td> <td>   -2.773</td> <td> 0.006</td> <td>   -7.209</td> <td>   -1.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_;</th>                <td>   -1.5962</td> <td>    0.696</td> <td>   -2.295</td> <td> 0.022</td> <td>   -2.960</td> <td>   -0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_(</th>                <td>   -0.6276</td> <td>    0.413</td> <td>   -1.519</td> <td> 0.129</td> <td>   -1.437</td> <td>    0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_[</th>                <td>   -0.0500</td> <td>    0.979</td> <td>   -0.051</td> <td> 0.959</td> <td>   -1.969</td> <td>    1.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_!</th>                <td>    0.2124</td> <td>    0.061</td> <td>    3.471</td> <td> 0.001</td> <td>    0.092</td> <td>    0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_$</th>                <td>    5.7022</td> <td>    1.098</td> <td>    5.193</td> <td> 0.000</td> <td>    3.550</td> <td>    7.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_#</th>                <td>    1.2748</td> <td>    1.654</td> <td>    0.771</td> <td> 0.441</td> <td>   -1.966</td> <td>    4.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>capital_run_length_average</th> <td>    0.7272</td> <td>    0.786</td> <td>    0.925</td> <td> 0.355</td> <td>   -0.813</td> <td>    2.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>capital_run_length_longest</th> <td>    0.5299</td> <td>    0.448</td> <td>    1.182</td> <td> 0.237</td> <td>   -0.349</td> <td>    1.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>capital_run_length_total</th>   <td>    1.8490</td> <td>    0.311</td> <td>    5.948</td> <td> 0.000</td> <td>    1.240</td> <td>    2.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>about_money</th>                <td>    0.6960</td> <td>    0.117</td> <td>    5.926</td> <td> 0.000</td> <td>    0.466</td> <td>    0.926</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:                 2009\n",
       "Model:                            GLM   Df Residuals:                     1958\n",
       "Model Family:                Binomial   Df Model:                           50\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                    nan\n",
       "Date:                Mon, 30 Dec 2019   Deviance:                          nan\n",
       "Time:                        12:51:57   Pearson chi2:                 2.24e+04\n",
       "No. Iterations:                   100                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "const                         -5.0624      0.464    -10.904      0.000      -5.972      -4.152\n",
       "word_freq_make                -0.1299      0.293     -0.443      0.658      -0.705       0.445\n",
       "word_freq_address             -0.1619      0.131     -1.239      0.216      -0.418       0.094\n",
       "word_freq_all                 -0.0884      0.167     -0.531      0.595      -0.415       0.238\n",
       "word_freq_3d                 350.6953    8.8e+05      0.000      1.000   -1.73e+06    1.73e+06\n",
       "word_freq_our                  0.5344      0.120      4.464      0.000       0.300       0.769\n",
       "word_freq_over                 0.6943      0.353      1.965      0.049       0.002       1.387\n",
       "word_freq_remove               1.6118      0.329      4.895      0.000       0.966       2.257\n",
       "word_freq_internet             0.5349      0.148      3.621      0.000       0.245       0.824\n",
       "word_freq_order               -0.1082      0.304     -0.356      0.722      -0.703       0.487\n",
       "word_freq_mail                 0.0561      0.090      0.625      0.532      -0.120       0.232\n",
       "word_freq_receive             -0.5076      0.380     -1.334      0.182      -1.253       0.238\n",
       "word_freq_will                -0.1398      0.107     -1.308      0.191      -0.349       0.070\n",
       "word_freq_people              -0.0531      0.334     -0.159      0.874      -0.708       0.602\n",
       "word_freq_report               0.0120      0.307      0.039      0.969      -0.590       0.614\n",
       "word_freq_addresses            0.8487      0.802      1.058      0.290      -0.724       2.421\n",
       "word_freq_email                0.1439      0.147      0.978      0.328      -0.144       0.432\n",
       "word_freq_you                  0.1415      0.051      2.782      0.005       0.042       0.241\n",
       "word_freq_your                 0.1381      0.074      1.870      0.061      -0.007       0.283\n",
       "word_freq_font                 0.1757      0.279      0.630      0.529      -0.371       0.722\n",
       "word_freq_000                  1.0192      0.468      2.176      0.030       0.101       1.937\n",
       "word_freq_lab                -12.4533     10.458     -1.191      0.234     -32.950       8.044\n",
       "word_freq_labs                -1.6974      1.046     -1.623      0.105      -3.748       0.353\n",
       "word_freq_telnet              -0.2793      1.874     -0.149      0.882      -3.952       3.394\n",
       "word_freq_857                 -0.5357      3.463     -0.155      0.877      -7.323       6.252\n",
       "word_freq_data                -0.4946      0.292     -1.696      0.090      -1.066       0.077\n",
       "word_freq_415                424.0571   4.52e+07   9.38e-06      1.000   -8.86e+07    8.86e+07\n",
       "word_freq_85                  -1.8880      1.518     -1.244      0.214      -4.863       1.087\n",
       "word_freq_technology           0.8304      0.547      1.519      0.129      -0.241       1.902\n",
       "word_freq_1999                 0.2841      0.237      1.197      0.231      -0.181       0.749\n",
       "word_freq_parts                0.3967      2.682      0.148      0.882      -4.861       5.654\n",
       "word_freq_pm                  -1.1317      0.562     -2.015      0.044      -2.233      -0.031\n",
       "word_freq_direct              -0.3583      0.445     -0.806      0.421      -1.230       0.513\n",
       "word_freq_cs                -354.6653   3.85e+06  -9.21e-05      1.000   -7.55e+06    7.55e+06\n",
       "word_freq_meeting             -6.4294      2.328     -2.762      0.006     -10.993      -1.866\n",
       "word_freq_original            -1.4478      1.044     -1.387      0.165      -3.493       0.598\n",
       "word_freq_project             -1.7884      0.827     -2.164      0.030      -3.408      -0.168\n",
       "word_freq_re                  -0.7498      0.195     -3.842      0.000      -1.132      -0.367\n",
       "word_freq_edu                 -4.7957      0.948     -5.059      0.000      -6.654      -2.938\n",
       "word_freq_table               -1.4227      2.442     -0.583      0.560      -6.210       3.364\n",
       "word_freq_conference          -4.2235      1.523     -2.773      0.006      -7.209      -1.239\n",
       "char_freq_;                   -1.5962      0.696     -2.295      0.022      -2.960      -0.233\n",
       "char_freq_(                   -0.6276      0.413     -1.519      0.129      -1.437       0.182\n",
       "char_freq_[                   -0.0500      0.979     -0.051      0.959      -1.969       1.869\n",
       "char_freq_!                    0.2124      0.061      3.471      0.001       0.092       0.332\n",
       "char_freq_$                    5.7022      1.098      5.193      0.000       3.550       7.855\n",
       "char_freq_#                    1.2748      1.654      0.771      0.441      -1.966       4.516\n",
       "capital_run_length_average     0.7272      0.786      0.925      0.355      -0.813       2.267\n",
       "capital_run_length_longest     0.5299      0.448      1.182      0.237      -0.349       1.409\n",
       "capital_run_length_total       1.8490      0.311      5.948      0.000       1.240       2.458\n",
       "about_money                    0.6960      0.117      5.926      0.000       0.466       0.926\n",
       "==============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamM1 = GLM(train.Class, add_constant(train.loc[:,:'about_money']), family=Binomial())\n",
    "resultM1 = spamM1.fit()\n",
    "resultM1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables/features that have small weights are probably not very relevant. Even though we can't do stepwise like in R (Statsmodels does not have stepwise variable selection) we can use crossvalidated Recursive Forward Elimination (RFE) with the implementation of logistic regression from scikit learn. RFE does the same thing as stepwise variable selection but uses accuracy to select the best model using cross validation. Here L1 is used as regularization to make 0 a large number of the weigths and the lower the C the more attributes will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear',penalty='l1',C=1)\n",
    "rfe = RFECV(estimator=logreg,cv=10,n_jobs=-1) \n",
    "rfe.fit(train.loc[:,:'about_money'],train.Class);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Selected: 37\n",
      "\n",
      " Ranking of features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>ranking</th>\n",
       "      <th>selected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>word_freq_data</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>word_freq_technology</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>word_freq_1999</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>word_freq_pm</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>word_freq_direct</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>word_freq_cs</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>word_freq_meeting</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>word_freq_original</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>word_freq_project</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>word_freq_85</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>word_freq_re</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>word_freq_conference</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>char_freq_;</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>char_freq_(</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>char_freq_!</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>char_freq_$</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>char_freq_#</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>capital_run_length_average</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>capital_run_length_longest</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>word_freq_edu</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>capital_run_length_total</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>about_money</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>word_freq_will</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>word_freq_receive</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>word_freq_labs</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>word_freq_over</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>word_freq_our</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>word_freq_addresses</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>word_freq_email</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>word_freq_remove</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>word_freq_you</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>word_freq_font</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>word_freq_000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>word_freq_lab</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>word_freq_address</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>word_freq_3d</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>word_freq_internet</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>word_freq_your</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>word_freq_mail</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>word_freq_all</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>word_freq_make</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>char_freq_[</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>word_freq_parts</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>word_freq_order</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>word_freq_table</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>word_freq_report</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>word_freq_857</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>word_freq_telnet</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>word_freq_415</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>word_freq_people</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      features  ranking  selected\n",
       "24              word_freq_data        1      True\n",
       "27        word_freq_technology        1      True\n",
       "28              word_freq_1999        1      True\n",
       "30                word_freq_pm        1      True\n",
       "31            word_freq_direct        1      True\n",
       "32                word_freq_cs        1      True\n",
       "33           word_freq_meeting        1      True\n",
       "34          word_freq_original        1      True\n",
       "35           word_freq_project        1      True\n",
       "26                word_freq_85        1      True\n",
       "36                word_freq_re        1      True\n",
       "39        word_freq_conference        1      True\n",
       "40                 char_freq_;        1      True\n",
       "41                 char_freq_(        1      True\n",
       "43                 char_freq_!        1      True\n",
       "44                 char_freq_$        1      True\n",
       "45                 char_freq_#        1      True\n",
       "46  capital_run_length_average        1      True\n",
       "47  capital_run_length_longest        1      True\n",
       "37               word_freq_edu        1      True\n",
       "48    capital_run_length_total        1      True\n",
       "49                 about_money        1      True\n",
       "11              word_freq_will        1      True\n",
       "10           word_freq_receive        1      True\n",
       "21              word_freq_labs        1      True\n",
       "5               word_freq_over        1      True\n",
       "4                word_freq_our        1      True\n",
       "14         word_freq_addresses        1      True\n",
       "15             word_freq_email        1      True\n",
       "6             word_freq_remove        1      True\n",
       "16               word_freq_you        1      True\n",
       "18              word_freq_font        1      True\n",
       "19               word_freq_000        1      True\n",
       "20               word_freq_lab        1      True\n",
       "1            word_freq_address        1      True\n",
       "3                 word_freq_3d        1      True\n",
       "7           word_freq_internet        1      True\n",
       "17              word_freq_your        2     False\n",
       "9               word_freq_mail        3     False\n",
       "2                word_freq_all        4     False\n",
       "0               word_freq_make        5     False\n",
       "42                 char_freq_[        6     False\n",
       "29             word_freq_parts        7     False\n",
       "8              word_freq_order        8     False\n",
       "38             word_freq_table        9     False\n",
       "13            word_freq_report       10     False\n",
       "23               word_freq_857       11     False\n",
       "22            word_freq_telnet       12     False\n",
       "25               word_freq_415       13     False\n",
       "12            word_freq_people       14     False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Features Selected:',rfe.n_features_)\n",
    "print('\\n Ranking of features')\n",
    "sel = pd.DataFrame({'features': train.columns[:-1], 'ranking': rfe.ranking_, \n",
    "                    'selected':rfe.support_})\n",
    "sel.sort_values(by='ranking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the extimator from the RFE and the list of selected variable to slice the data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_freq_address',\n",
       " 'word_freq_3d',\n",
       " 'word_freq_our',\n",
       " 'word_freq_over',\n",
       " 'word_freq_remove',\n",
       " 'word_freq_internet',\n",
       " 'word_freq_receive',\n",
       " 'word_freq_will',\n",
       " 'word_freq_addresses',\n",
       " 'word_freq_email',\n",
       " 'word_freq_you',\n",
       " 'word_freq_font',\n",
       " 'word_freq_000',\n",
       " 'word_freq_lab',\n",
       " 'word_freq_labs',\n",
       " 'word_freq_data',\n",
       " 'word_freq_85',\n",
       " 'word_freq_technology',\n",
       " 'word_freq_1999',\n",
       " 'word_freq_pm',\n",
       " 'word_freq_direct',\n",
       " 'word_freq_cs',\n",
       " 'word_freq_meeting',\n",
       " 'word_freq_original',\n",
       " 'word_freq_project',\n",
       " 'word_freq_re',\n",
       " 'word_freq_edu',\n",
       " 'word_freq_conference',\n",
       " 'char_freq_;',\n",
       " 'char_freq_(',\n",
       " 'char_freq_!',\n",
       " 'char_freq_$',\n",
       " 'char_freq_#',\n",
       " 'capital_run_length_average',\n",
       " 'capital_run_length_longest',\n",
       " 'capital_run_length_total',\n",
       " 'about_money']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultM1 = rfe.estimator_\n",
    "sel_features = list(sel.features[sel.selected])\n",
    "sel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the probability is at least 0.5, we can predict spam:\n",
    "def spam_acc(P=0.5):\n",
    "    # Accuracy in training\n",
    "    pred = resultM1.predict_proba(train.loc[:,sel_features])[:,1]\n",
    "    lab_tr = [1 if i>=P else 0 for i in pred]\n",
    "    df_tr=confusion(train.Class,lab_tr, classes=['nospam','spam'])\n",
    "\n",
    "    # Accuracy in test\n",
    "    pred = resultM1.predict_proba(test.loc[:,sel_features])[:,1]\n",
    "    lab_ts = [1 if i>=P else 0 for i in pred]\n",
    "    df_ts=confusion(test.Class,lab_ts, classes=['nospam','spam'])\n",
    " \n",
    "    return df_tr, (1-accuracy_score(train.Class,lab_tr))*100,\\\n",
    "           df_ts, (1-accuracy_score(test.Class,lab_ts))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>nospam</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>nospam</td>\n",
       "      <td>760</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>71</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  nospam  spam\n",
       "Actual                 \n",
       "nospam        760    89\n",
       "spam           71  1089"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 7.964161274265802%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>nospam</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>nospam</td>\n",
       "      <td>381</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>34</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  nospam  spam\n",
       "Actual                 \n",
       "nospam        381    42\n",
       "spam           34   533"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 7.676767676767682%\n"
     ]
    }
   ],
   "source": [
    "c_tr,e_tr,c_ts,e_ts= spam_acc()\n",
    "c_tr\n",
    "print(f'Training error: {e_tr}%')\n",
    "c_ts\n",
    "print(f'Test error: {e_ts}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to lower the probability of predicting spam when it is not (at the expense of increasing the converse probability) by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>nospam</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>nospam</td>\n",
       "      <td>803</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>161</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  nospam  spam\n",
       "Actual                 \n",
       "nospam        803    46\n",
       "spam          161   999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 10.303633648581378%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>nospam</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>nospam</td>\n",
       "      <td>402</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>81</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  nospam  spam\n",
       "Actual                 \n",
       "nospam        402    21\n",
       "spam           81   486"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 10.303030303030303%\n"
     ]
    }
   ],
   "source": [
    "c_tr,e_tr,c_ts,e_ts= spam_acc(0.7)\n",
    "c_tr\n",
    "print(f'Training error: {e_tr}%')\n",
    "c_ts\n",
    "print(f'Test error: {e_ts}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get a much better spam filter; notice that the filter has a very low probability of \n",
    "predicting spam when it is not (which is the delicate case), of about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.964539007092199"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_ts.loc['nospam','spam'] /c_ts.loc['nospam'].sum()*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
